{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25365,"status":"ok","timestamp":1732336262105,"user":{"displayName":"Yuta Hayashi","userId":"16353596730523707040"},"user_tz":-540},"id":"NeKrhsB3Q_3H","outputId":"55bf2b22-a2b4-4f95-9c6a-1d90511946d2"},"outputs":[],"source":["# Google Colabの場合はunslothのインストールのみを行ってください\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip uninstall unsloth\n","!pip install unsloth"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"elapsed":17331,"status":"error","timestamp":1732336279432,"user":{"displayName":"Yuta Hayashi","userId":"16353596730523707040"},"user_tz":-540},"id":"LbmtYWUH8p_J","outputId":"6cbc14f7-5e29-4f39-9528-779f6cdf1636"},"outputs":[],"source":["# llm-jp/llm-jp-3-13bを4bit量子化のqLoRA設定でロード。\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 512 # unslothではRoPEをサポートしているのでコンテキスト長は自由に設定可能\n","dtype = None # Noneにしておけば自動で設定\n","load_in_4bit = True # 今回は8Bクラスのモデルを扱うためTrue\n","\n","model_id = \"llm-jp/llm-jp-3-13b\"\n","new_model_id = \"llm-jp-3-13b-SFT-LoRA\" #Fine-Tuningしたモデルにつけたい名前\n","# FastLanguageModel インスタンスを作成\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=model_id,\n","    dtype=dtype,\n","    load_in_4bit=load_in_4bit,\n","    trust_remote_code=True,\n",")\n","\n","# SFT用のモデルを用意\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 32,\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 32,\n","    lora_dropout = 0.05,\n","    bias = \"none\",\n","    use_gradient_checkpointing = \"unsloth\",\n","    random_state = 3407,\n","    use_rslora = False,\n","    loftq_config = None,\n","    max_seq_length = max_seq_length,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1732336027811,"user":{"displayName":"Yuta Hayashi","userId":"16353596730523707040"},"user_tz":-540},"id":"WAaS0RKXgG72"},"outputs":[],"source":["# Hugging Faceで取得したTokenをこちらに貼る。\n","HF_TOKEN = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RT0wnFkYjNpO"},"outputs":[],"source":["import json\n","from datasets import Dataset\n","\n","# JSONファイルを手動で読み込む\n","with open(\"./combined_dataset.json\", \"r\", encoding=\"utf-8-sig\") as f:\n","    data = json.load(f)\n","\n","# データの内容を確認\n","print(\"Loaded data:\", data[:5])  # 最初の5件を表示\n","\n","# データセットに変換（Noneを空文字列に変換）\n","dataset = Dataset.from_dict({\n","    \"instruction\": [str(item[\"instruction\"]) for item in data],\n","    \"input\": [str(item[\"input\"]) if item[\"input\"] is not None else \"\" for item in data],\n","    \"output\": [str(item[\"output\"]) for item in data]\n","})\n","\n","# データセットを1万件に絞る\n","dataset = dataset.select(range(10000))\n","\n","# データセットの形式を確認\n","print(\"Dataset format:\", dataset[\"train\"].features)\n","\n","def preprocess_function(examples):\n","    prompts = []\n","    for instruction, input_text in zip(examples[\"instruction\"], examples[\"input\"]):\n","        # input_textが空または None の場合は instruction のみを使用\n","        if input_text and input_text.strip():\n","            # instructionとinput_textを明確に区分して結合\n","            prompt = f\"指示：{instruction}\\n\\n入力：{input_text}\"\n","        else:\n","            prompt = f\"指示：{instruction}\"\n","        prompts.append(prompt)\n","    \n","    return {\n","        \"text\": prompts,\n","        \"output\": examples[\"output\"]\n","    }\n","\n","# データセットの変換\n","processed_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n","\n","# デバッグ用に最初の数個のプロンプトを表示\n","print(processed_dataset[\"text\"][:5])\n","\n","# データセットの変換\n","processed_dataset = dataset[\"train\"].map(preprocess_function)\n","processed_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BANlYJFSKf-K"},"outputs":[],"source":["# 学習時のプロンプトフォーマットの定義\n","prompt = \"\"\"### 指示\n","{}\n","### 回答\n","{}\"\"\"\n","\n","\n","\n","\"\"\"\n","formatting_prompts_func: 各データをプロンプトに合わせた形式に合わせる\n","\"\"\"\n","EOS_TOKEN = tokenizer.eos_token # トークナイザーのEOSトークン（文末トークン）\n","def formatting_prompts_func(examples):\n","    input = examples[\"text\"] # 入力データ\n","    output = examples[\"output\"] # 出力データ\n","    text = prompt.format(input, output) + EOS_TOKEN # プロンプトの作成\n","    return { \"formatted_text\" : text, } # 新しいフィールド \"formatted_text\" を返す\n","pass\n","\n","# # 各データにフォーマットを適用\n","processed_dataset = processed_dataset.map(\n","    formatting_prompts_func,\n","    num_proc= 4, # 並列処理数を指定\n",")\n","\n","processed_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NhF0GLaTkUZ"},"outputs":[],"source":["# データを確認\n","print(processed_dataset[\"train\"][\"formatted_text\"][3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHSyrGcEQ_3I"},"outputs":[],"source":["\"\"\"\n","training_arguments: 学習の設定\n","\n","  - output_dir:\n","      -トレーニング後のモデルを保存するディレクトリ\n","\n","  - per_device_train_batch_size:\n","      - デバイスごとのトレーニングバッチサイズ\n","\n","  - per_device_eval_batch_size:\n","      - デバイスごとの評価バッチサイズ\n","\n","  - gradient_accumulation_steps:\n","      - 勾配を更新する前にステップを積み重ねる回数\n","\n","  - optim:\n","      - オプティマイザの設定\n","\n","  - num_train_epochs:\n","      - エポック数\n","\n","  - eval_strategy:\n","      - 評価の戦略 (\"no\"/\"steps\"/\"epoch\")\n","\n","  - eval_steps:\n","      - eval_strategyが\"steps\"のとき、評価を行うstep間隔\n","\n","  - logging_strategy:\n","      - ログ記録の戦略\n","\n","  - logging_steps:\n","      - ログを出力するステップ間隔\n","\n","  - warmup_steps:\n","      - 学習率のウォームアップステップ数\n","\n","  - save_steps:\n","      - モデルを保存するステップ間隔\n","\n","  - save_total_limit:\n","      - 保存しておくcheckpointの数\n","\n","  - max_steps:\n","      - トレーニングの最大ステップ数\n","\n","  - learning_rate:\n","      - 学習率\n","\n","  - fp16:\n","      - 16bit浮動小数点の使用設定（第8回演習を参考にすると良いです）\n","\n","  - bf16:\n","      - BFloat16の使用設定\n","\n","  - group_by_length:\n","      -  入力シーケンスの長さによりバッチをグループ化 (トレーニングの効率化)\n","\n","  - report_to:\n","      - ログの送信先 (\"wandb\"/\"tensorboard\"など)\n","\"\"\"\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset=processed_dataset,\n","    max_seq_length = max_seq_length,\n","    dataset_text_field=\"formatted_text\",\n","    packing = False,\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        num_train_epochs = 1,\n","        logging_steps = 10,\n","        warmup_steps = 10,\n","        save_steps=100,\n","        save_total_limit=2,\n","        max_steps=-1,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        group_by_length=True,\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3U8FUkwTx_K"},"outputs":[],"source":["#@title 現在のメモリ使用量を表示\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjrchK4hSr0b"},"outputs":[],"source":["#@title 学習実行\n","trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_P7-b0_Q_3J"},"outputs":[],"source":["# ELYZA-tasks-100-TVの読み込み。事前にファイルをアップロードしてください\n","# データセットの読み込み。\n","# omnicampusの開発環境では、左にタスクのjsonlをドラッグアンドドロップしてから実行。\n","import json\n","datasets = []\n","with open(\"./elyza-tasks-100-TV_0.jsonl\", \"r\") as f:\n","    item = \"\"\n","    for line in f:\n","      line = line.strip()\n","      item += line\n","      if item.endswith(\"}\"):\n","        datasets.append(json.loads(item))\n","        item = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsbwPAOJQ_3J"},"outputs":[],"source":["# 学習したモデルを用いてタスクを実行\n","from tqdm import tqdm\n","\n","# 推論するためにモデルのモードを変更\n","FastLanguageModel.for_inference(model)\n","\n","results = []\n","for dt in tqdm(datasets):\n","  input = dt[\"input\"]\n","\n","  prompt = f\"\"\"### 入力\\n{input}\\n### 回答\\n\"\"\"\n","\n","  inputs = tokenizer([prompt], return_tensors = \"pt\").to(model.device)\n","\n","  outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, do_sample=False, repetition_penalty=1.2)\n","  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).split('\\n### 回答')[-1]\n","\n","  results.append({\"task_id\": dt[\"task_id\"], \"input\": input, \"output\": prediction})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pd8VbKeIQ_3J"},"outputs":[],"source":["# jsonlで保存\n","with open(f\"{new_model_id}_output.jsonl\", 'w', encoding='utf-8') as f:\n","    for result in results:\n","        json.dump(result, f, ensure_ascii=False)\n","        f.write('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zq4Ko1FWakX9"},"outputs":[],"source":["# モデルとトークナイザーをHugging Faceにアップロード。\n","# 一旦privateでアップロードしてください。\n","# 最終成果物が決まったらpublicにするようお願いします。\n","# 現在公開しているModel_Inference_Template.ipynbはunslothを想定していないためそのままでは動かない可能性があります。\n","model.push_to_hub_merged(\n","    new_model_id,\n","    tokenizer=tokenizer,\n","    save_method=\"lora\",\n","    token=HF_TOKEN,\n","    private=True\n",")\n","\n","# model.push_to_hub(new_model_id, token=HF_TOKEN, private=True) # Online saving\n","# tokenizer.push_to_hub(new_model_id, token=HF_TOKEN) # Online saving"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
